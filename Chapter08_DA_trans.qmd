---
title: "第8章"
subtitle: "AI(深層学習)すでに学習されたモデルGPT2を使う"
author: "ここは自分の名前に変える"
format: docx
jupyter: python3
editor: source
always_allow_html: true
---
```{python}
# 必要なライブラリ
%pip install -Uq transformers torch
```

```{python}
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
from datetime import datetime

# Current time
print("[1]", datetime.now())
```

```{python}
model_name = "gpt2" 

print(f"Loading {model_name}...")

# 1. トークナイザー（プリプロセッサ）の読み込み
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

```{python}
# 2. モデルの読み込み
model = AutoModelForCausalLM.from_pretrained(model_name)

# 3. テキスト生成の実行
input_text = "Data science is "
input_ids = tokenizer.encode(input_text, return_tensors="pt")

print("Generating text...")
start = time.time()

# 生成設定
output = model.generate(
    input_ids, 
    max_length=128, 
    num_return_sequences=1,
    no_repeat_ngram_size=2, # 同じフレーズの繰り返しを防ぐ
    do_sample=True,         # ランダム性を持たせる
    temperature=0.8
)

end = time.time()
```

```{python}
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(f"\nGenerated text (Time: {end - start:.2f}s):")
print(generated_text)
```
