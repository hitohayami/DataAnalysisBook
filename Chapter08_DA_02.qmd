---
title: "第8章"
subtitle: "MNISTS by keras using Python"
author: "自分の名前に変える"
format: docx
jupyter: python3
editor: source
always_allow_html: true
---

## 準備

1.  古いPythonのバージョンがある場合は削除しておくか，自己責任でpyenvなどの環境を整える必要がある．

2.  Python 3.12.10をインストールする (python 3.14.1はkerasをインポートできない)
    anacondaを使っている人は自己責任でconda仮想環境を整備する

3.  pipないしはpip3でkeras, jax, jupyter, matplotlibをインストールする

    pip3 install --upgrade keras jax flax jupyter matplotlib scikit-learn tfds-nightly seaborn tensorflow --quiet

4. VS Codeの場合は，Pythonのみを使うので, 6. へ

5.  RStudioを開く．Packagesのupgrade allをしておく

6.  RStudioのTools -\> Global Options... -\> Python -\> Select Mac OS の場合 /usr/local/bin/python3 selectをクリック -\> 画面が戻る Apply をクリック -\> Ok をクリック -\> 画面が戻る Restarting R session と窓が出る． Yesと答える．

7.  実行にはパソコンの性能によって，20分かそれ以上かかる場合がある．私のMac M2の場合，1 epocに100秒くらいかかって，20 epocでラーニングするので25分はかかった．最後にテストして終わる．

## keras によるMNIST

```{python}
#| label: 念のためkears3 と jax の アップグレード・インストール
#| message: false
#| include: false
%pip3 install --upgrade --quiet keras jax flax jupyter matplotlib scikit-learn tfds-nightly seaborn tensorflow --quiet
```

```{python}
import os
os.environ["JAX_PLATFORMS"] = "cpu"

import jax
import jax.numpy as jnp
from flax import linen as nn
from flax.training import train_state
import optax
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# --- 1. モデル定義 ---
class SimpleCNN(nn.Module):
    @nn.compact
    def __call__(self, x):
        x = nn.Conv(features=16, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = nn.Conv(features=32, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = x.reshape((x.shape[0], -1))
        x = nn.Dense(features=64)(x)
        x = nn.relu(x)
        x = nn.Dense(features=10)(x)
        return x

# --- 2. ステップ定義 ---
@jax.jit
def train_step(state, batch_x, batch_y):
    def loss_fn(params):
        logits = SimpleCNN().apply({'params': params}, batch_x)
        loss = optax.softmax_cross_entropy(logits=logits, labels=jax.nn.one_hot(batch_y, 10)).mean()
        return loss, logits
    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
    (_, logits), grads = grad_fn(state.params)
    state = state.apply_gradients(grads=grads)
    return state, _, jnp.mean(jnp.argmax(logits, -1) == batch_y)

@jax.jit
def predict_with_probs(state, batch_x):
    logits = SimpleCNN().apply({'params': state.params}, batch_x)
    return jax.nn.softmax(logits) # 確率（0~1）を返す
@jax.jit
def eval_step(state, batch_x, batch_y):
    logits = SimpleCNN().apply({'params': state.params}, batch_x)
    loss = optax.softmax_cross_entropy(logits=logits, labels=jax.nn.one_hot(batch_y, 10)).mean()
    acc = jnp.mean(jnp.argmax(logits, -1) == batch_y)
    return loss, acc
@jax.jit
def predict_step(state, batch_x):
    logits = SimpleCNN().apply({'params': state.params}, batch_x)
    return jnp.argmax(logits, axis=-1)

# --- 3. 分析用関数 ---

def plot_only_errors(x_test, y_test, y_pred, num=24):
    """間違えた画像だけを表示"""
    error_indices = np.where(y_pred != y_test)[0]
    selected_indices = np.random.choice(error_indices, min(num, len(error_indices)), replace=False)
    
    plt.figure(figsize=(15, 10))
    for i, idx in enumerate(selected_indices):
        plt.subplot(4, 6, i + 1)
        plt.imshow(x_test[idx].reshape(28, 28), cmap='magma') # 目立つように色を変更
        plt.title(f"True:{y_test[idx]} | Pred:{y_pred[idx]}", color='red', fontsize=10)
        plt.axis('off')
    plt.suptitle(f"Misclassified Examples (Total Errors: {len(error_indices)})", fontsize=16)
    plt.show()

def plot_digit_confidence(y_test, y_probs, target_digit=5):
    """特定の数字に対する自信度の分布を表示"""
    # target_digit である画像のみ抽出
    target_indices = np.where(y_test == target_digit)[0]
    # その画像に対してモデルが target_digit だと思った確率
    confidences = y_probs[target_indices, target_digit]
    
    plt.figure(figsize=(8, 5))
    plt.hist(confidences, bins=20, color='skyblue', edgecolor='black')
    plt.title(f"Confidence Distribution for Digit '{target_digit}'")
    plt.xlabel("Confidence (Probability)")
    plt.ylabel("Number of Samples")
    plt.grid(axis='y', alpha=0.3)
    plt.show()

# --- 4. メインルーチン ---
def main():
    from tensorflow.keras.datasets import mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0
    x_test = x_test.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0

    state = train_state.TrainState.create(
        apply_fn=SimpleCNN().apply, 
        params=SimpleCNN().init(jax.random.PRNGKey(0), jnp.ones([1, 28, 28, 1]))['params'],
        tx=optax.adam(1e-3))
    history = {"train_loss": [], "train_acc": [], "test_loss": [], "test_acc": []}
    batch_size = 128
    epochs = 5
    print("Training 5 epochs...")
    for epoch in range(epochs):
        idx = np.random.permutation(len(x_train))
        t_losses, t_accs = [], []
        for i in range(0, len(x_train), batch_size):
            state, l, a = train_step(state, x_train[idx][i:i+128], y_train[idx][i:i+128])
            t_losses.append(l); t_accs.append(a)
        v_losses, v_accs = [], []
        for i in range(0, len(x_test), batch_size):
            l, a = eval_step(state, x_test[i:i+batch_size], y_test[i:i+batch_size])
            v_losses.append(l); v_accs.append(a)

        history["train_loss"].append(np.mean(t_losses))
        history["train_acc"].append(np.mean(t_accs))
        history["test_loss"].append(np.mean(v_losses))
        history["test_acc"].append(np.mean(v_accs))
        print(f"Epoch {epoch+1}: Train Acc={history['train_acc'][-1]:.4f}, Test Acc={history['test_acc'][-1]:.4f}")

    # 全テストデータの予測確率を取得
    y_probs = np.concatenate([predict_with_probs(state, x_test[i:i+128]) for i in range(0, len(x_test), 128)])
    y_pred = np.argmax(y_probs, axis=1)

# --- 可視化 1: 学習曲線 ---
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    ax1.plot(history["train_loss"], label="Train Loss", color="blue", linestyle="--")
    ax1.plot(history["test_loss"], label="Test Loss", color="red")
    ax1.set_title("Loss Improvement"); ax1.set_xlabel("Epoch"); ax1.legend(); ax1.grid(True)
    
    ax2.plot(history["train_acc"], label="Train Acc", color="blue", linestyle="--")
    ax2.plot(history["test_acc"], label="Test Acc", color="red")
    ax2.set_title("Accuracy Improvement"); ax2.set_xlabel("Epoch"); ax2.legend(); ax2.grid(True)
    plt.tight_layout()
    plt.show()

  # --- 可視化 2: 予測プレビュー (24枚) ---
    
    indices = np.random.choice(len(x_test), 24, replace=False)
    samples_x, samples_y = x_test[indices], y_test[indices]
    preds = predict_step(state, samples_x)
    
    plt.figure(figsize=(15, 10))
    for i in range(24):
        plt.subplot(4, 6, i + 1)
        plt.imshow(samples_x[i].reshape(28, 28), cmap='gray')
        color = 'blue' if preds[i] == samples_y[i] else 'red'
        plt.title(f"Pred: {preds[i]}\nTrue: {samples_y[i]}", color=color)
        plt.axis('off')
    plt.suptitle("Sample Predictions (Blue: Correct, Red: Incorrect)", fontsize=16)
    plt.tight_layout()
    plt.show()
  
    # 分析1: 自信度の分布（例として「5」を分析）
    plot_digit_confidence(y_test, y_probs, target_digit=5)

    # 分析2: 間違えた画像だけを24枚表示
    plot_only_errors(x_test, y_test, y_pred, num=24)

    # 分析3: 混同行列
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title("Final Confusion Matrix")
    plt.show()

if __name__ == "__main__":
    main()
```